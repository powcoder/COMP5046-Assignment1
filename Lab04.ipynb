{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JgAANQLNyE3p"
   },
   "source": [
    "# Lab04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0vysAG7fyIQJ"
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HEqu0hg8DaEh"
   },
   "source": [
    "\n",
    "## Predict a last character of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uw5s2SVGU9qI"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "char_arr = ['a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
    "            'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
    "            'o', 'p', 'q', 'r', 's', 't', 'u',\n",
    "            'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "# one-hot encoding and decoding \n",
    "# {'a': 0, 'b': 1, 'c': 2, ..., 'j': 9, 'k', 10, ...}\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "dic_len = len(num_dic)\n",
    "\n",
    "\n",
    "# a list words for sequence data (input and output)\n",
    "seq_data = ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n",
    "\n",
    "# Make a batch to have sequence data for input and ouput\n",
    "# wor -> X, d -> Y\n",
    "# dee -> X, p -> Y\n",
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for seq in seq_data:\n",
    "        # input data is:\n",
    "        #     wor           woo        dee       div\n",
    "        # [22, 14, 17] [22, 14, 14] [3, 4, 4] [3, 8, 21] ...\n",
    "        \n",
    "        input_data = [num_dic[n] for n in seq[:-1]]\n",
    "        \n",
    "        # target is :\n",
    "        # d, d, p, e, ...\n",
    "        # 3, 3, 15, 4, ...\n",
    "        target = num_dic[seq[-1]]\n",
    "        \n",
    "        # convert input to one-hot encoding.\n",
    "        # if input is [3, 4, 4]:\n",
    "        # [[ 0,  0,  0,  1,  0,  0,  0, ... 0]\n",
    "        #  [ 0,  0,  0,  0,  1,  0,  0, ... 0]\n",
    "        #  [ 0,  0,  0,  0,  1,  0,  0, ... 0]]\n",
    "        input_batch.append(np.eye(dic_len)[input_data])\n",
    "        \n",
    "        # sparse_softmax_cross_entropy_with_logits() will be used for cost function, does not require to convert to one-hot vector\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return input_batch, target_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MwcYilPL1jK0"
   },
   "source": [
    "### **[softmax_cross_entropy_with_logits_v2()](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2) vs [sparse_softmax_cross_entropy_with_logits()](https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits) **\n",
    "\n",
    "**softmax_cross_entropy_with_logits_v2()**: While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution. If they are not, the computation of the gradient will be incorrect.\n",
    "\n",
    "**sparse_softmax_cross_entropy_with_logits()**: For this operation, the probability of a given label is considered exclusive. That is, soft classes are not allowed, and the labels vector must provide a single specific index for the true class for each row of logits (each minibatch entry). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9hp5TkPM1gxa"
   },
   "outputs": [],
   "source": [
    "### Setting hyperparameters\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_hidden = 128\n",
    "total_epoch = 50\n",
    "\n",
    "# Number of sequences for RNN\n",
    "n_step = 3\n",
    "\n",
    "# number of inputs (dimension of input vector) = 26\n",
    "n_input = dic_len\n",
    "# number of classes = 26\n",
    "n_class = dic_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cfcWRs-C4zcr"
   },
   "outputs": [],
   "source": [
    "### Neural Network Model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_step, n_input])\n",
    "\n",
    "# Again, here we are using sparse_softmax_cross_entropy_with_logits() for cost function, so the output is not one-hot vector\n",
    "# if we are getting one-hot vector shape should be: [None, n_class]\n",
    "Y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "\n",
    "W = tf.Variable(tf.random_normal([n_hidden, n_class]))\n",
    "b = tf.Variable(tf.random_normal([n_class]))\n",
    "\n",
    "# Create a cell for RNN \n",
    "cell1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "\n",
    "# Apply Dropout to prevent overfitting\n",
    "cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5)\n",
    "cell2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "# RNN cell composed sequentially of multiple simple cells\n",
    "multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "\n",
    "# tf.nn.dynamic_rnn \n",
    "# time_major=True\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)\n",
    "\n",
    "# Convert output to one-hot vector\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "outputs = outputs[-1]\n",
    "model = tf.matmul(outputs, W) + b\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=model, labels=Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n0NPv3hP7Oou"
   },
   "source": [
    "### Dropout\n",
    "[DropoutWrapper](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/DropoutWrapper)\n",
    "\n",
    "Dropout makes each hidden unit more robust and drive it towards creating useful features on its own without relying on other hidden units to correct its mistakes\n",
    "\n",
    "![dropout](https://cdn-images-1.medium.com/max/800/1*D8jriroKkjno8RztHKmMnA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 969
    },
    "colab_type": "code",
    "id": "lZrwHQ3g4oAK",
    "outputId": "702611bd-a85b-4311-f4c8-b46aa5e7b7a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 4.129886\n",
      "Epoch: 0002 cost = 3.138178\n",
      "Epoch: 0003 cost = 1.817054\n",
      "Epoch: 0004 cost = 1.625509\n",
      "Epoch: 0005 cost = 0.878741\n",
      "Epoch: 0006 cost = 0.996808\n",
      "Epoch: 0007 cost = 0.600500\n",
      "Epoch: 0008 cost = 0.592670\n",
      "Epoch: 0009 cost = 0.354496\n",
      "Epoch: 0010 cost = 0.359257\n",
      "Epoch: 0011 cost = 0.352133\n",
      "Epoch: 0012 cost = 0.250323\n",
      "Epoch: 0013 cost = 0.317002\n",
      "Epoch: 0014 cost = 0.364188\n",
      "Epoch: 0015 cost = 0.252094\n",
      "Epoch: 0016 cost = 0.103315\n",
      "Epoch: 0017 cost = 0.112947\n",
      "Epoch: 0018 cost = 0.125758\n",
      "Epoch: 0019 cost = 0.128380\n",
      "Epoch: 0020 cost = 0.153149\n",
      "Epoch: 0021 cost = 0.066520\n",
      "Epoch: 0022 cost = 0.065893\n",
      "Epoch: 0023 cost = 0.036547\n",
      "Epoch: 0024 cost = 0.031514\n",
      "Epoch: 0025 cost = 0.015861\n",
      "Epoch: 0026 cost = 0.016400\n",
      "Epoch: 0027 cost = 0.090019\n",
      "Epoch: 0028 cost = 0.049094\n",
      "Epoch: 0029 cost = 0.051274\n",
      "Epoch: 0030 cost = 0.018310\n",
      "Epoch: 0031 cost = 0.010033\n",
      "Epoch: 0032 cost = 0.011302\n",
      "Epoch: 0033 cost = 0.008801\n",
      "Epoch: 0034 cost = 0.008848\n",
      "Epoch: 0035 cost = 0.002120\n",
      "Epoch: 0036 cost = 0.077387\n",
      "Epoch: 0037 cost = 0.009328\n",
      "Epoch: 0038 cost = 0.055019\n",
      "Epoch: 0039 cost = 0.002583\n",
      "Epoch: 0040 cost = 0.010524\n",
      "Epoch: 0041 cost = 0.010423\n",
      "Epoch: 0042 cost = 0.003332\n",
      "Epoch: 0043 cost = 0.002175\n",
      "Epoch: 0044 cost = 0.000613\n",
      "Epoch: 0045 cost = 0.000399\n",
      "Epoch: 0046 cost = 0.000590\n",
      "Epoch: 0047 cost = 0.043259\n",
      "Epoch: 0048 cost = 0.000373\n",
      "Epoch: 0049 cost = 0.002302\n",
      "Epoch: 0050 cost = 0.000728\n",
      "Learning completed\n",
      "\n",
      "=== Prediction Result ===\n",
      "Input: ['wor ', 'woo ', 'dee ', 'div ', 'col ', 'coo ', 'loa ', 'lov ', 'kis ', 'kin ']\n",
      "Predicted: ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "### Train Model\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "input_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost],\n",
    "                       feed_dict={X: input_batch, Y: target_batch})\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1),\n",
    "          'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "print('Training completed')\n",
    "\n",
    "### Prediction Result\n",
    "# Convert predict result to integer\n",
    "prediction = tf.cast(tf.argmax(model, 1), tf.int32)\n",
    "# Compare predicted result with actual result\n",
    "prediction_check = tf.equal(prediction, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction_check, tf.float32))\n",
    "\n",
    "input_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "predict, accuracy_val = sess.run([prediction, accuracy],\n",
    "                                 feed_dict={X: input_batch, Y: target_batch})\n",
    "\n",
    "predict_words = []\n",
    "for idx, val in enumerate(seq_data):\n",
    "    last_char = char_arr[predict[idx]]\n",
    "    predict_words.append(val[:3] + last_char)\n",
    "\n",
    "print('\\n=== Prediction Result ===')\n",
    "print('Input:', [w[:3] + ' ' for w in seq_data])\n",
    "print('Predicted:', predict_words)\n",
    "print('Accuracy:', accuracy_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KPDUdScFPRzT"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XfRXWwL2D9Bn"
   },
   "source": [
    "# Seq2Seq Model (N to M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UFRHe5J9c3QG"
   },
   "source": [
    "We are going to implement a sequence to sequence model that translates playing card symbols (Ace, Jack, Queen, King) to their associated number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1yEhcJZxxWvu"
   },
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MyTFAjAYlIgn"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Sequence data\n",
    "seq_data = [['ace', '01'], ['jack', '11'],\n",
    "            ['queen', '12'], ['king', '13']]\n",
    "\n",
    "# Generate unique tokens list\n",
    "chars = []\n",
    "for seq in seq_data:    \n",
    "    chars += list(seq[0])\n",
    "    chars += list(seq[1])\n",
    "\n",
    "char_arr = list(set(chars))\n",
    "\n",
    "# special tokens are required\n",
    "# B: Beginning of Sequence\n",
    "# E: Ending of Sequence\n",
    "# P: Padding of Sequence - for different size input\n",
    "# U: Unknown element of Sequence - for different size input\n",
    "char_arr.append('B')\n",
    "char_arr.append('E')\n",
    "char_arr.append('P')\n",
    "char_arr.append('U')\n",
    "\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "\n",
    "dic_len = len(num_dic)\n",
    "\n",
    "max_input_words_amount = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gKk4ORyCxbjE"
   },
   "source": [
    "## Generate batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNPoWhYDIL71"
   },
   "outputs": [],
   "source": [
    "\n",
    "# add paddings if the word is shorter than the maximum number of words\n",
    "def add_paddings(word):\n",
    "    diff = 5 - len(word)\n",
    "    for x in range(diff):\n",
    "        word += \"P\"\n",
    "    return word\n",
    "    \n",
    "\n",
    "# generate a batch data for training/testing\n",
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        # Input for encoder cell, convert to vector\n",
    "        input_word = add_paddings(seq[0])\n",
    "        input_data = [num_dic[n] for n in input_word]\n",
    "        \n",
    "        # Input for decoder cell, Add 'B' at the beginning of the sequence data\n",
    "        output_data  = [num_dic[n] for n in ('B'+ seq[1])]\n",
    "        \n",
    "        # Output of decoder cell (Actual result), Add 'E' at the end of the sequence data\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "\n",
    "        # Convert each character vector to one-hot encode data\n",
    "        input_batch.append(np.eye(dic_len)[input_data])\n",
    "        output_batch.append(np.eye(dic_len)[output_data])\n",
    "        \n",
    "        target_batch.append(target)\n",
    "\n",
    "    return input_batch, output_batch, target_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "US15fGgPxgBf"
   },
   "source": [
    "## Build training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "esUeOZ6KIQol"
   },
   "outputs": [],
   "source": [
    "### Neural Network Model\n",
    "\n",
    "### Setting Hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_hidden = 128\n",
    "total_epoch = 100\n",
    "\n",
    "n_class = dic_len\n",
    "n_input = dic_len\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# encoder/decoder shape = [batch size, time steps, input size]\n",
    "enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "\n",
    "# target shape = [batch size, time steps]\n",
    "targets = tf.placeholder(tf.int64, [None, None])\n",
    "\n",
    "\n",
    "# Encoder Cell\n",
    "with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "\n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
    "                                            dtype=tf.float32)\n",
    "# Decoder Cell\n",
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "\n",
    "    # [IMPORTANT] Setting enc_states as inital_state of decoder cell\n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
    "                                            initial_state=enc_states,\n",
    "                                            dtype=tf.float32)\n",
    "\n",
    "model = tf.layers.dense(outputs, n_class, activation=None)\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=model, labels=targets))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L30V5F6jxqR-"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "yG7KGxcOxsCc",
    "outputId": "7eb3ff03-675b-40bd-bdaf-8d33a95d5157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 2.811764\n",
      "Epoch: 0011 cost = 0.100158\n",
      "Epoch: 0021 cost = 0.003316\n",
      "Epoch: 0031 cost = 0.002131\n",
      "Epoch: 0041 cost = 0.000990\n",
      "Epoch: 0051 cost = 0.000265\n",
      "Epoch: 0061 cost = 0.000108\n",
      "Epoch: 0071 cost = 0.000176\n",
      "Epoch: 0081 cost = 0.000454\n",
      "Epoch: 0091 cost = 0.000064\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "### Training Model\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Generate a batch data\n",
    "input_batch, output_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "# print(target_batch)\n",
    "# for target_each in target_batch : \n",
    "#     print(len(target_each))\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost],\n",
    "                       feed_dict={enc_input: input_batch,\n",
    "                                  dec_input: output_batch,\n",
    "                                  targets: target_batch})\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1),\n",
    "              'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "print('Training completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXf-4bByxgh8"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "bT0TjoPqF7qa",
    "outputId": "5fb32186-90cf-432f-c039-b29399de07ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prediction result ===\n",
      "ace -> 01\n",
      "jack -> 11\n",
      "queen -> 12\n",
      "king -> 13\n"
     ]
    }
   ],
   "source": [
    "### Evaluation\n",
    "\n",
    "# Predict the result \n",
    "def predict(word):\n",
    "    # Setting each character of predicted as 'U' (Unknown) \n",
    "    # ['king', 'UU']\n",
    "    word = add_paddings(word)\n",
    "    \n",
    "    seq_data = [word, 'U' * 2]\n",
    "\n",
    "    input_batch, output_batch, target_batch = make_batch([seq_data])\n",
    "    \n",
    "    prediction = tf.argmax(model, 2)\n",
    "\n",
    "    result = sess.run(prediction,\n",
    "                      feed_dict={enc_input: input_batch,\n",
    "                                 dec_input: output_batch,\n",
    "                                 targets: target_batch})\n",
    "\n",
    "    # convert index number to actual character \n",
    "    decoded = [char_arr[i] for i in result[0]]\n",
    "\n",
    "    # Remove anything after 'E' \n",
    "    end = decoded.index('E')\n",
    "    translated = ''.join(decoded[:end])\n",
    "\n",
    "    return translated\n",
    "\n",
    "\n",
    "print('=== Prediction result ===')\n",
    "print('ace ->', predict('ace'))\n",
    "print('jack ->', predict('jack'))\n",
    "print('queen ->', predict('queen'))\n",
    "print('king ->', predict('king'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-316qjIt-zS"
   },
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ka3S9SD1uDl4"
   },
   "source": [
    "You are required to implement Seq2Seq model chatbot. We are going to use [Microsoft Personality Chat Datasets](https://github.com/Microsoft/BotBuilder-PersonalityChat/tree/63dd818cc22ed5a84f7b77c88076809c0b77a88d/CSharp/Datasets) (Google Drive id is provided). \n",
    "\n",
    "Use \"Question\" and \"Answer\" data in the tsv file. We will be implementing Many-to-One Seq2Seq model and feed word-based (tokenised) input rather than character based.\n",
    "\n",
    "Fill the blank to complete the program\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-hlDQBqsV1L"
   },
   "source": [
    "### Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "jvwyvmfDegwE",
    "outputId": "5ec112e6-6ff5-49b1-cb1f-bf33268c0236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# You should submit \"ipynb\" file (You can download it from \"File\" > \"Download .ipynb\") to Canvas\n",
    "import json\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Code to download file into Colaboratory:\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "id = '1zMofHuFOuc6FJ3ndj19VO5lTyWGA0QDJ'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('qna_chitchat_the_professional.tsv')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GcwnJS0BsiEo"
   },
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQPN9-Sysluc"
   },
   "outputs": [],
   "source": [
    "# Sequence data\n",
    "# Generate unique tokens list from qas.json\n",
    "seq_data = []\n",
    "whole_words = []\n",
    "max_input_words_amount = 0\n",
    "max_output_words_amount = 1\n",
    "\n",
    "\n",
    "df = pd.read_csv('qna_chitchat_the_professional.tsv', sep=\"\\t\")\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    ###<You need to fill here>###\n",
    "    question = \n",
    "    answer = \n",
    "    \n",
    "    seq_data.append(      )\n",
    "    ###</You need to fill here>###\n",
    "    \n",
    "    \n",
    "    ###<You need to fill here>###\n",
    "    # we need to tokenise question    \n",
    "    tokenized_q = \n",
    "    \n",
    "    # we do not need to tokenise answer (because we implement N to One model)\n",
    "    # make a list with only one element (whole sentence)\n",
    "    tokenized_a = \n",
    "    \n",
    "    ###</You need to fill here>###\n",
    "    \n",
    "    \n",
    "    # add question list and answer list (one element)\n",
    "    whole_words += tokenized_q\n",
    "    whole_words += tokenized_a\n",
    "    \n",
    "    # we need to decide the maximum size of input word tokens\n",
    "    max_input_words_amount = max(len(tokenized_q), max_input_words_amount)\n",
    "    \n",
    "\n",
    "# we now have a vacabulary list\n",
    "unique_words = list(set(whole_words))\n",
    "\n",
    "# adding special tokens in the vocabulary list    \n",
    "# _B_: Beginning of Sequence\n",
    "# _E_: Ending of Sequence\n",
    "# _P_: Padding of Sequence - for different size input\n",
    "# _U_: Unknown element of Sequence - for different size input\n",
    "\n",
    "unique_words.append('_B_')\n",
    "unique_words.append('_E_')\n",
    "unique_words.append('_P_')\n",
    "unique_words.append('_U_')\n",
    "\n",
    "\n",
    "num_dic = {n: i for i, n in enumerate(unique_words)}\n",
    "dic_len = len(num_dic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrAL53HKufcG"
   },
   "source": [
    "## Generate batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n4vtcmaZufk2"
   },
   "outputs": [],
   "source": [
    "# get token index vector of questions and add paddings if the word is shorter than the maximum number of words\n",
    "def get_vectors_q(sentence):\n",
    "    \n",
    "    # tokenise the sentence\n",
    "    ###<You need to fill here>###\n",
    "    tokenized_sentence = \n",
    "    ###</You need to fill here>###    \n",
    "    \n",
    "    diff = max_input_words_amount - len(tokenized_sentence)\n",
    "    \n",
    "    # add paddings if the word is shorter than the maximum number of words    \n",
    "    for x in range(diff):\n",
    "        ###<You need to fill here>###\n",
    "        tokenized_sentence.append(          )\n",
    "        \n",
    "        ###</You need to fill here>###            \n",
    "        \n",
    "    ###<You need to fill here>###\n",
    "    data = \n",
    "    ###</You need to fill here>###      \n",
    "    \n",
    "        \n",
    "    return data\n",
    "\n",
    "# get token index vector of answer\n",
    "def get_vectors_a(sentence):    \n",
    "    tokenized_sentence = [sentence]\n",
    "    data = tokens_to_ids(tokenized_sentence)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "\n",
    "# convert tokens to index\n",
    "def tokens_to_ids(tokenized_sentence):\n",
    "    ids = []\n",
    "\n",
    "    for token in tokenized_sentence:\n",
    "        ###<You need to fill here>###\n",
    "        if token in num_dic:\n",
    "            ids.append(               )\n",
    "        else:\n",
    "            ids.append(               )\n",
    "        ###</You need to fill here>###      \n",
    "\n",
    "    return ids\n",
    "\n",
    "\n",
    "# generate a batch data for training/testing\n",
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for seq in seq_data:        \n",
    "        # Input for encoder cell, convert question to vector\n",
    "        ###<You need to fill here>###\n",
    "        input_data = \n",
    "        ###</You need to fill here>###      \n",
    "        \n",
    "        # Input for decoder cell, Add '_B_' at the beginning of the sequence data\n",
    "        ###<You need to fill here>###\n",
    "        output_data = [num_dic[                     ]]\n",
    "        ###</You need to fill here>###   \n",
    "        output_data += get_vectors_a(seq[1])\n",
    "        \n",
    "        # Output of decoder cell (Actual result), Add '_E_' at the end of the sequence data\n",
    "        ###<You need to fill here>###\n",
    "        target = \n",
    "        target.append(                             )\n",
    "        ###</You need to fill here>###   \n",
    "        \n",
    "        # Convert each token vector to one-hot encode data\n",
    "        input_batch.append(np.eye(dic_len)[input_data])\n",
    "        output_batch.append(np.eye(dic_len)[output_data])\n",
    "        \n",
    "        target_batch.append(target)\n",
    "\n",
    "    return input_batch, output_batch, target_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_W08vIPv7L7"
   },
   "source": [
    "## Build training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4o1hzIzuDvj"
   },
   "outputs": [],
   "source": [
    "### Setting Hyperparameters\n",
    "learning_rate = 0.002\n",
    "n_hidden = 128\n",
    "\n",
    "n_class = dic_len\n",
    "n_input = dic_len\n",
    "\n",
    "### Neural Network Model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# encoder/decoder shape = [batch size, time steps, input size]\n",
    "enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "\n",
    "# target shape = [batch size, time steps]\n",
    "targets = tf.placeholder(tf.int64, [None, None])\n",
    "\n",
    "\n",
    "# Encoder Cell\n",
    "with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "\n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
    "                                            dtype=tf.float32)\n",
    "# Decoder Cell\n",
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "\n",
    "    # [IMPORTANT] Setting enc_states as inital_state of decoder cell\n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
    "                                            initial_state=enc_states,\n",
    "                                            dtype=tf.float32)\n",
    "\n",
    "model = tf.layers.dense(outputs, n_class, activation=None)\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=model, labels=targets))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    \n",
    "### Training Model\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Generate a batch data\n",
    "input_batch, output_batch, target_batch = make_batch(seq_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJIcvSB2w_rs"
   },
   "source": [
    "## Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxsUy2hQOvl4"
   },
   "outputs": [],
   "source": [
    "total_epoch = 5000\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost],\n",
    "                       feed_dict={enc_input: input_batch,\n",
    "                                  dec_input: output_batch,\n",
    "                                  targets: target_batch})\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1),\n",
    "              'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "print('Epoch:', '%04d' % (epoch + 1),\n",
    "      'cost =', '{:.6f}'.format(loss))\n",
    "print('Training completed')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RT2GVA2L04r4"
   },
   "source": [
    "\n",
    "### Output:\n",
    "```\n",
    "Epoch: 0001 cost = 6.520396\n",
    "Epoch: 0101 cost = 2.285382\n",
    "Epoch: 0201 cost = 2.246728\n",
    "Epoch: 0301 cost = 2.245017\n",
    "Epoch: 0401 cost = 2.246025\n",
    "Epoch: 0501 cost = 2.240861\n",
    "Epoch: 0601 cost = 2.243242\n",
    "Epoch: 0701 cost = 2.242677\n",
    "Epoch: 0801 cost = 2.237896\n",
    "Epoch: 0901 cost = 2.231663\n",
    "Epoch: 1001 cost = 2.235167\n",
    "Epoch: 1101 cost = 2.241715\n",
    "Epoch: 1201 cost = 2.231011\n",
    "Epoch: 1301 cost = 2.239064\n",
    "Epoch: 1401 cost = 2.239094\n",
    "Epoch: 1501 cost = 2.238503\n",
    "Epoch: 1601 cost = 2.240443\n",
    "Epoch: 1701 cost = 1.363001\n",
    "Epoch: 1801 cost = 1.003514\n",
    "Epoch: 1901 cost = 0.825378\n",
    "Epoch: 2001 cost = 0.740003\n",
    "Epoch: 2101 cost = 0.740753\n",
    "Epoch: 2201 cost = 0.584820\n",
    "Epoch: 2301 cost = 0.520495\n",
    "Epoch: 2401 cost = 0.475480\n",
    "Epoch: 2501 cost = 0.451525\n",
    "Epoch: 2601 cost = 0.447824\n",
    "Epoch: 2701 cost = 0.391890\n",
    "Epoch: 2801 cost = 0.372204\n",
    "Epoch: 2901 cost = 0.364561\n",
    "Epoch: 3001 cost = 0.325550\n",
    "Epoch: 3101 cost = 0.328401\n",
    "Epoch: 3201 cost = 0.278636\n",
    "Epoch: 3301 cost = 0.287389\n",
    "Epoch: 3401 cost = 0.242130\n",
    "Epoch: 3501 cost = 0.226778\n",
    "Epoch: 3601 cost = 0.197740\n",
    "Epoch: 3701 cost = 0.351003\n",
    "Epoch: 3801 cost = 0.170044\n",
    "Epoch: 3901 cost = 0.138550\n",
    "Epoch: 4001 cost = 0.139902\n",
    "Epoch: 4101 cost = 0.119088\n",
    "Epoch: 4201 cost = 0.111903\n",
    "Epoch: 4301 cost = 0.117569\n",
    "Epoch: 4401 cost = 0.115386\n",
    "Epoch: 4501 cost = 0.089837\n",
    "Epoch: 4601 cost = 0.107535\n",
    "Epoch: 4701 cost = 0.094628\n",
    "Epoch: 4801 cost = 0.079053\n",
    "Epoch: 4901 cost = 0.762524\n",
    "Epoch: 5000 cost = 0.083978\n",
    "Training completed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-OmjFDS5wICj"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "356ED2TFNMq7"
   },
   "outputs": [],
   "source": [
    "### Evaluation\n",
    "\n",
    "# Answer the question using the trained model\n",
    "def answer(sentence):\n",
    "    \n",
    "    seq_data = [sentence, '_U_' * max_output_words_amount]\n",
    "\n",
    "    input_batch, output_batch, target_batch = make_batch([seq_data])\n",
    "    \n",
    "    prediction = tf.argmax(model, 2)\n",
    "\n",
    "    result = sess.run(prediction,\n",
    "                      feed_dict={enc_input: input_batch,\n",
    "                                 dec_input: output_batch,\n",
    "                                 targets: target_batch})\n",
    "\n",
    "    # convert index number to actual token \n",
    "    decoded = [unique_words[i] for i in result[0]]\n",
    "        \n",
    "    # Remove anything after '_E_'        \n",
    "    if \"_E_\" in decoded:\n",
    "        end = decoded.index('_E_')\n",
    "        translated = ' '.join(decoded[:end])\n",
    "    else :\n",
    "        translated = ' '.join(decoded[:])\n",
    "    \n",
    "    return translated\n",
    "\n",
    "questions = [\"Hello\",\"I am so lonely\", \"Can you sleep?\", \"What is your age?\", \"I hate you\", \"Do you like me?\", \"You're so mean\", \"Can you drive?\", \"That's so bad\", \"what do you mean?\", \"oh my god\"]\n",
    "for q in questions:\n",
    "    print(q , ' ->', answer(q))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tgDIypjS07yD"
   },
   "source": [
    "\n",
    "### Output \n",
    "\n",
    "```\n",
    "Hello  -> Hello.\n",
    "I am so lonely  -> Okay.\n",
    "Can you sleep?  -> I don't have a body.\n",
    "What is your age?  -> Age doesn't really apply to me. \n",
    "I hate you  -> I'm sorry to hear that.\n",
    "Do you like me?  -> I do like you.\n",
    "You're so mean  -> I aim for efficiency.\n",
    "That's so bad  -> It's nice to have things you love.\n",
    "what do you mean?  -> Sorry about that.\n",
    "oh my god  -> I hope you're able to get some rest soon.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YsFtFn_-NRcW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab04.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
