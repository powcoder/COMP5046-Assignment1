{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNIKEY_COMP5046_Ass1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "MGHoy6KpQDfZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# COMP5046 Assignment 1\n",
        "*Make sure you change the file name with your unikey.*"
      ]
    },
    {
      "metadata": {
        "id": "qTf21j_oQIiD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the user, please mention here.* \n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style*"
      ]
    },
    {
      "metadata": {
        "id": "iXbQohXLKSgO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***Visualising the comparison of different results is a good way to justify your decision.***"
      ]
    },
    {
      "metadata": {
        "id": "34DVNKgqQY21",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1 - Data Preprocessing (Personality chat datasets)"
      ]
    },
    {
      "metadata": {
        "id": "7cWUxAQrGlq6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1. Download Dataset (Personality chat datasets)"
      ]
    },
    {
      "metadata": {
        "id": "U7C4snIcNl22",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# links of 3 csv files\n",
        "link_professional = 'https://drive.google.com/open?id=1oPxUI1xxdye5-tIdokYjLvYIRfFOhGD-' \n",
        "link_friend = 'https://drive.google.com/open?id=1TZTWQnfQaySF72PRxp3PEuZ8kdF4v_Ti' \n",
        "link_comic = 'https://drive.google.com/open?id=1BukHDfwEbxxK8Df7jWRhXIDksjyf8HWq' \n",
        "\n",
        "# names of files\n",
        "names=['qna_chitchat_the_professional.csv', 'qna_chitchat_the_friend.csv', 'qna_chitchat_the_comic.csv']\n",
        "\n",
        "# dictionary to store file names and corresponding content in dataframe format\n",
        "dic={}\n",
        "\n",
        "# store links to obtain relevant ids\n",
        "files = [link_professional, link_friend, link_comic]\n",
        "for i in range(0, len(files)):\n",
        "    fluff, id = files[i].split('=') \n",
        "    downloaded = drive.CreateFile({'id': id})\n",
        "    downloaded.GetContentFile(names[i])\n",
        "    dic[i]=pd.read_csv(names[i], sep=',', header=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l9gBSgBCQh24",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2. Preprocess data (Personality chat datasets)"
      ]
    },
    {
      "metadata": {
        "id": "8RdKI8E2KRwe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*You are required to describe which data preprocessing techniques were conducted with justification of your decision. *"
      ]
    },
    {
      "metadata": {
        "id": "Y3CQeNt4zZYf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "515b941b-d7f8-4524-d2d9-d6b2765adbcf"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "emyl1lWxGr12",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code\n",
        "\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"let's\": \"let us\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "def preprocess(tokens, punctuations=False, decapitalisation=False, remove_numbers=False,\n",
        "               stemming=False, lemmatising=False):\n",
        "    #remove punctuarions and emoticons \n",
        "    if punctuations:\n",
        "        tokens = [x for x in tokens if re.sub(r'[^\\w\\s]','',x)]\n",
        "    #decapitalisation of all words in a sentence\n",
        "    if decapitalisation:\n",
        "        tokens = [x.lower() for x in tokens]\n",
        "    #Stemming\n",
        "    if stemming:\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(x) for x in tokens]\n",
        "  \n",
        "    #Lemmatising\n",
        "    if lemmatising:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(x) for x in tokens]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "\n",
        "\n",
        "# remove puncutations\n",
        "\n",
        "def remove_punctuation_re(x):\n",
        "    x = re.sub(r'[^\\w\\s]','',x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "  \n",
        "# \n",
        "Questions = {}\n",
        "Answers = {}\n",
        "\n",
        "for id, text in dic.items():\n",
        "    # get questions and answers seperately\n",
        "    \n",
        "    Question=[]\n",
        "    Answer=[]\n",
        "    for i in range(len(text)):\n",
        "        Question.append(text[\"Question\"].loc[i])\n",
        "        Answer.append(text[\"Answer\"].loc[i])\n",
        "        \n",
        "        # replace contraction with words\n",
        "        for old, new in contraction_dict.items():\n",
        "            \n",
        "            Question[i] = Question[i].lower().replace(old, new)\n",
        "            Answer[i] = Answer[i].lower().replace(old, new)\n",
        "\n",
        "            \n",
        "        \n",
        "        # tokenization\n",
        "        Question[i] = word_tokenize(Question[i])\n",
        "        Answer[i] = word_tokenize(Answer[i])\n",
        "        \n",
        "        # remove \"?\" and \".\"\n",
        "        Question[i] = [token for token in Question[i] if token != \"?\"]\n",
        "        Answer[i] = [token for token in Answer[i] if token != \".\"]\n",
        "        \n",
        "        # preprocessing tokens\n",
        "        preprocess(Question[i],punctuations=True, decapitalisation=True, remove_numbers=True, \n",
        "                   stemming=True, lemmatising=True)\n",
        "        preprocess(Answer[i],punctuations=True, decapitalisation=True, remove_numbers=True,\n",
        "                   stemming=True, lemmatising=True)\n",
        "        \n",
        "        \n",
        "    Questions[id]=Question\n",
        "    Answers[id]=Answer\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LIu_lkJwQ55g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2 - Model Implementation"
      ]
    },
    {
      "metadata": {
        "id": "daDvAftceIvr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1. Word Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "lbzm-NWBTmM-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*You are required to describe which model was implemented (i.e. Word2Vec with CBOW, FastText with SkipGram, etc.) with justification of your decision *"
      ]
    },
    {
      "metadata": {
        "id": "3cM4rlYkHefJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "it6I1_K7HTub",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.1. Download Dataset for Word Embeddings\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4Op66omXKVHa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*You are required to describe which data was used with justification of your decision.*"
      ]
    },
    {
      "metadata": {
        "id": "QLjf_pm9NiA8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GXgFpxIgl-_G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.2. Data Preprocessing for Word Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "qJrVHGYSmYMg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*You are required to describe which preprocessing techniques were used with justification of your decision.*"
      ]
    },
    {
      "metadata": {
        "id": "3LByzHLiNinu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qhAgWf_AmbZ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.3. Build Word Embeddings Model"
      ]
    },
    {
      "metadata": {
        "id": "AJ8rU7JbiBVS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "metadata": {
        "id": "TVPuwWgvNjOU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LNys5HOdISK-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.4. Train Word Embeddings Model"
      ]
    },
    {
      "metadata": {
        "id": "Ae8i7Z2kIef-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uMCv3YI1IfUo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.5. Save Word Embeddings Model"
      ]
    },
    {
      "metadata": {
        "id": "3OwicNPkIqd1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yn16xrDrIs8B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.6. Load Word Embeddings Model"
      ]
    },
    {
      "metadata": {
        "id": "-IebpYFsIvgh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tlCeWT8eeLnd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.2. Seq2Seq model"
      ]
    },
    {
      "metadata": {
        "id": "fwA-NN3EJ4Ig",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.1. Apply/Import Word Embedding Model"
      ]
    },
    {
      "metadata": {
        "id": "UAMJrxx-iOVn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "metadata": {
        "id": "g7PKX1gIePA2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DpYCL17JKZxl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.2. Build Seq2Seq Model"
      ]
    },
    {
      "metadata": {
        "id": "R204UIyDKhZ4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "metadata": {
        "id": "13eCtR_SLUG6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6BaOiaGRLW7R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.3. Train Seq2Seq Model"
      ]
    },
    {
      "metadata": {
        "id": "lVQnUSX1LZ6C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-2feNpG-LZx2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.4. Save Seq2Seq Model"
      ]
    },
    {
      "metadata": {
        "id": "sflUAgV4L1o8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4zFo6YppL6w3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.5. Load Seq2Seq Model"
      ]
    },
    {
      "metadata": {
        "id": "OtNxLzDGMCan",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a4mpRpocePLN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3 - Evaluation (Running chatbot)"
      ]
    },
    {
      "metadata": {
        "id": "KEW1zMgVMREr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1. Start chatting"
      ]
    },
    {
      "metadata": {
        "id": "LPHCb-bneTI9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P28Z1k36MZuo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2. Change Personality"
      ]
    },
    {
      "metadata": {
        "id": "U8OBtJfvMgL_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Explain how to change personality (What is the command for changing personality?). *"
      ]
    },
    {
      "metadata": {
        "id": "wTLyQEeZMZ2f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y50Ep8KKMZ99",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.3. Save chat log"
      ]
    },
    {
      "metadata": {
        "id": "bbZ6oOu6MaGJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JISqR3jjMwwU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.4. End chatting"
      ]
    },
    {
      "metadata": {
        "id": "nT_DeoHSMw49",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HpomO_3YNI5X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.5. Execute program"
      ]
    },
    {
      "metadata": {
        "id": "cDkQJ9i_NH9D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***Please make sure your program  is running properly.***\n",
        "\n",
        "***Functions for downloading (from Google Drive) and loading models (both word embeddings and Seq2Seq) need to be called!*** \n"
      ]
    },
    {
      "metadata": {
        "id": "_7J5hS_SOIUU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.5.1. Execute program - training mode"
      ]
    },
    {
      "metadata": {
        "id": "_woLwuU3Mk3w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Please include lines to train the bot.*"
      ]
    },
    {
      "metadata": {
        "id": "xhWYz7NQOfLV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "65cZTuQ_OeI7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.5.2. Execute program - chatting mode"
      ]
    },
    {
      "metadata": {
        "id": "D7LrbcP_PKap",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Please include lines to start chatting with the bot.*"
      ]
    },
    {
      "metadata": {
        "id": "QVvzZsB7PbYf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sfv8rWTKPzeb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Object Oriented Programming codes here"
      ]
    },
    {
      "metadata": {
        "id": "TS23AjBRSZaX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*If you have multiple classes use multiple code snippets to add them.*"
      ]
    },
    {
      "metadata": {
        "id": "wSJJ4zRFQy1h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# If you used OOP style, use this sectioon"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}